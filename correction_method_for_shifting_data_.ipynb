{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJOEc3+IokS675RQFNXF4I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivanshu04/Case_Based_code/blob/main/correction_method_for_shifting_data_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaU9xCTlkfBr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def transform_dataframe(df):\n",
        "    # For each row, find the first non-NaN value from columns 3 onwards and replace NaN in the first two columns\n",
        "    for index, row in df.iterrows():\n",
        "        non_na_values = row[2:].dropna().values\n",
        "\n",
        "        if pd.isna(row.iloc[0]) and len(non_na_values) > 0:\n",
        "            df.at[index, df.columns[0]] = non_na_values[0]\n",
        "            non_na_values = np.delete(non_na_values, 0)\n",
        "\n",
        "        if pd.isna(row.iloc[1]) and len(non_na_values) > 0:\n",
        "            df.at[index, df.columns[1]] = non_na_values[0]\n",
        "            non_na_values = np.delete(non_na_values, 0)\n",
        "\n",
        "        # Update the remaining columns with the leftover values from non_na_values\n",
        "        for col_index, col in enumerate(df.columns[2:], start=2):\n",
        "            if col_index - 2 < len(non_na_values):\n",
        "                df.at[index, col] = non_na_values[col_index - 2]\n",
        "            else:\n",
        "                df.at[index, col] = np.nan\n",
        "\n",
        "    return df\n",
        "\n",
        "def process_file(file_path, output_folder):\n",
        "    # Read the CSV file into a DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Apply the transformation logic\n",
        "    df = transform_dataframe(df)\n",
        "\n",
        "    # Save the modified DataFrame to a new CSV file\n",
        "    output_path = os.path.join(output_folder, os.path.basename(file_path))\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "# List of file paths to process\n",
        "file_paths = [\"\", \"/path/to/file2.csv\", ...]\n",
        "\n",
        "# Output folder to save modified files\n",
        "output_folder = \"/path/to/output/folder\"\n",
        "\n",
        "# Create output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Process each file\n",
        "for file_path in file_paths:\n",
        "    process_file(file_path, output_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import necessary libraries\n",
        "import tabula\n",
        "import pandas as pd\n",
        "\n",
        "# 2. Function to consolidate data based on primary columns\n",
        "def consolidate_data(tables):\n",
        "    # Define the primary columns\n",
        "    primary_columns = [\"Signatory ID\", \"Name\", \"CIN\", \"Company Name\", \"Defaulting Year\"]\n",
        "\n",
        "    # Create an empty DataFrame with the primary columns\n",
        "    consolidated_df = pd.DataFrame(columns=primary_columns)\n",
        "\n",
        "    # Iterate over each table and extract rows that contain the primary columns\n",
        "    for table in tables:\n",
        "        for _, row in table.iterrows():\n",
        "            # Check if the primary columns are present in the row\n",
        "            if any(col in row.values for col in primary_columns):\n",
        "                # Extract relevant data and append to the consolidated DataFrame\n",
        "                row_data = {col: row[col] if col in row else None for col in primary_columns}\n",
        "                consolidated_df = consolidated_df.append(row_data, ignore_index=True)\n",
        "\n",
        "    return consolidated_df\n",
        "\n",
        "# 3. Function to extract tables from PDF and consolidate the data\n",
        "def convert_pdf_to_excel_consolidated(pdf_path, output_path):\n",
        "    # Extract tables from the PDF\n",
        "    tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
        "\n",
        "    # Consolidate the extracted data\n",
        "    consolidated_table = consolidate_data(tables)\n",
        "\n",
        "    # Write the consolidated table to Excel\n",
        "    consolidated_table.to_excel(output_path, sheet_name=\"Consolidated_Data\", index=False)\n",
        "\n",
        "    print(f\"Data exported to {output_path}\")\n",
        "\n",
        "# 4. Example Usage (replace paths with your actual paths)\n",
        "# pdf_path = \"path_to_your_pdf_file.pdf\"\n",
        "# output_excel_path = \"path_for_output_excel_file.xlsx\"\n",
        "# convert_pdf_to_excel_consolidated(pdf_path, output_excel_path)\n"
      ],
      "metadata": {
        "id": "Mbjeif5RR4FX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}